{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teeview Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to build a simple analytics tool that allows us to keep a tab on the Teespring campaigns indexed by teeview that have potential to be successfull in order to get inspiration only from this subset. \n",
    "\n",
    "The final outcome should initially be a bot (think Slack) that informs us when a new potential successfull campaign is identified in order to jump on it right away and draw inspiration from it. The purpose of the bot is to get near real time notifications since in this cases timing is fundamental.\n",
    "\n",
    "The principles that will be used to identify the campaigns that have success potential which are the ones we need to draw attention to is basic physics principles. We will get the velocity of the campaign and its acceleration using the numerical derivative. This will produce 3 plots which are the position, velocity and acceleration of a campaign. While a successfull campaign has a good final position it should have had high velocity values and high acceleration values to reach such velocities. This is the case because the length of the campaigns is fixed. That is the equivalent to having a sprint in runners where there is a defined distance beforehand.\n",
    "\n",
    "Given this context the goal is to identify potentially successfull campaigns i.e. noticing campaigns that have the potential to be successfull before they actually succeed (since one they succeed the market will be saturated of that design and as such it will no longer represent an opportunity).\n",
    "\n",
    "In order to make this identification one needs to check the acceleration value in conjunction with the velocity. Think that there are at least 2 cases in which you would definitely would want to get a piece of the action.\n",
    "\n",
    "1. Acceleration is practically null or even negative but velocity is still quite high.\n",
    "2. Velocity is still slow but acceleration is really high.\n",
    "\n",
    "There should be a coefficient that would indicate if we should jump on a campaign or not. To be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to crawl [https://www.teeview.org] and get the latest campaigns. Let's define the \"latest campaigns\" as the ones which have been added during the last day. We should only select those which say:\n",
    "\n",
    "1. \"31 sold, available until [Thursday!]\"\n",
    "2. \"Only 3 more needed to print!\" \n",
    "\n",
    "In order to do this we need to get the url of the campaign and go to [http://www.teespring.com] to see if this message appears on the campaign. If it does then we have data to populate our database and this url goes into the list of urls which we will follow periodically along with the time ago it was added, we will run the script periodically to get the sales data and be able to make the position, velocity and acceleration plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Database Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 2 database tables. \n",
    "\n",
    "One of them will store the information of relevant campaigns, a relevant campaign is defined as that which is obtained through the \"teeview_scraper\" function and posteriorly filtered through the \"campaigns_updater\" function (which gets all campaigns that are added within one day ago and filters only the ones which report sales data. This table will be called \"Campaigns\"\n",
    "\n",
    "The second database table will consist of the entries produced by the \"sales_data_updater\" function which will query Teespring for each campaign in \"Campaigns\" and add the latest sales data as an entry to the table. This will be the data that will be used to plot position, velocity and acceleration. This table will be called \"SalesData\"\n",
    "\n",
    "The database as a whole should be cleaned every day or so to keep it short and running smoothly. The cleaning process will consist of going through the database to see which campaigns have ended and removing the respective rows from the \"Campaign\" and \"SalesData\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function creates a database instance which consists of 2 tables. One called \"campaigns\"\n",
    "# which stores the information of new and active campaigns which report sales data and one\n",
    "# called \"sales_data\" which stores the sales data information of such campaigns\n",
    "def create_database():\n",
    "    print(\"started creating database...\")\n",
    "    # Connect to \"teeview_analytics\" database\n",
    "    conn = sqlite3.connect('teeview_analytics.db')\n",
    "    # Create \"campaigns\" table if it does not exist\n",
    "    campaigns_table = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='campaigns'\").fetchall()\n",
    "    if len(campaigns_table) == 0: conn.execute(\"create table campaigns(url, name, img)\")\n",
    "    # Create \"sales_data\" table if it does not exist\n",
    "    campaigns_table = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sales_data'\").fetchall()\n",
    "    if len(campaigns_table) == 0: conn.execute(\"create table sales_data(campaign_url, sales, timestamp)\")\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"finished creating database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will clean the database removing the entries in both tables i.e. \"campaigns\" and\n",
    "# \"sales_data\" which are associated to campaigns that have already ended\n",
    "def clean_database():\n",
    "    print(\"starting cleaning database...\")\n",
    "    # Connect to \"teeview_analytics\" database\n",
    "    conn = sqlite3.connect('teeview_analytics.db')\n",
    "    # Loop through \"campaigns\" table\n",
    "    for row in conn.execute(\"SELECT url FROM campaigns\").fetchall():\n",
    "        campaign_url = row[0]\n",
    "        response = requests.get(campaign_url)\n",
    "        html = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Text of relevant html elements of campaign\n",
    "        sales_data_text = html.select(\".persistent_timer__order_count\")[0].getText().lower()\n",
    "        ended_campaign_text = html.select(\".persistent_timer__stats--relaunchable\")\n",
    "        # Delete associated rows in \"campaigns\" and \"sales_data\" tables if campaign has ended\n",
    "        if ended_campaign_text or (\"only\" not in sales_data_text and \"sold\" not in sales_data_text):\n",
    "            print(\"DELETE CAMPAIGN WITH URL: {0}\".format(campaign_url))\n",
    "            conn.execute(\"DELETE FROM campaigns WHERE url='{0}'\".format(campaign_url))\n",
    "            conn.execute(\"DELETE FROM sales_data WHERE campaign_url='{0}'\".format(campaign_url))\n",
    "    # Commit changes to connection\n",
    "    conn.commit()\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"finished cleaning database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define JSON Config File Read & Update Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to update configuration JSON file to store the \"latest_campaign_link\", the \"last_query_timestamp\" and any other variable that needs to be stored persistently but does not constitute an entry in any of the database tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updates the config file with the \"latest_campaign_link\" and the \"last_query_timestamp\" \n",
    "def update_config(latest_campaign_link, last_query_timestamp):\n",
    "    # Create config data object\n",
    "    data = {\"LATEST_CAMPAIGN_LINK\": latest_campaign_link, \"LAST_QUERY_TIMESTAMP\": last_query_timestamp, \"SLEEP_INTERVAL\": 3600}\n",
    "    # Write config data object into config.json\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads config JSON file and return the Python Object Representation\n",
    "def read_config():\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this section the main functions used by the program to fetch the data periodically are defined, namely:\n",
    "\n",
    "1. **teeview_scraper():** This function is the one responsible for scraping the links out of [http://www.teeview.org] that are within a day ago or which are younger than the youngest link of the last query.\n",
    "2. **campaigns_updater():** This function has as a parameter the links obtained by the **teeview_scraper()** function and its purpose is to go through each one of these links and only get the appropriate info on those campaigns which report sales data and for each of these campaigns make the appropriate entry to the \"campaigns\" database table.\n",
    "3. **sales_data_updater():** This function has no parameters. Is runned after the **campaigns_updater()** function and it loops through the \"campaigns\" database table and queries such campaigns in order to extract the latest sales reported data and make the appropriate entries to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will make successive get requests to teeview and return the new campaigns that were added\n",
    "# within the last day or sooner than the latest campaign saved on the last query latest_campaign_link\n",
    "def teeview_scraper():\n",
    "    print(\"starting teeview_scraper...\")\n",
    "    # Set variables of: first \"page\" to query, query while data is \"whithin_day\" and array to store \"teeview_data\" \n",
    "    page = 1\n",
    "    within_day = True\n",
    "    teeview_data = []\n",
    "    config_data = read_config()\n",
    "    # Loop all campaign that were added within one day ago\n",
    "    while within_day:\n",
    "        # Form url of teeview made up of active campaigns on page \"page\" \n",
    "        url = \"https://www.teeview.org/site/index?active=true&page={0}&per-page=12\".format(page)\n",
    "        # Make request to get the page of teeview and parse it\n",
    "        response = requests.get(url)\n",
    "        html = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Get links of teespring campaigns of the page queried\n",
    "        for thumbnail in html.select(\".thumbnail\"):\n",
    "            campaign_url = thumbnail.select(\"h3 a\")[0]['href']\n",
    "            campaign_time_ago = thumbnail.select(\"p.text-muted small\")[0].getText()\n",
    "            # Break the loop if we have reached the campaigns that were added longer than a day ago\n",
    "            if \"day\" in campaign_time_ago:\n",
    "                within_day = False\n",
    "                break\n",
    "            # Break the loop if we have reached the latest_campaign_queried\n",
    "            if config_data[\"LATEST_CAMPAIGN_LINK\"] == campaign_url:\n",
    "                within_day = False\n",
    "                break\n",
    "            # Add the campaign data to teeview_data\n",
    "            teeview_data.extend([campaign_url])\n",
    "        # Increase page counter\n",
    "        page += 1\n",
    "    print(\"finished teeview_scraper!\")\n",
    "    # Returns the gathered \"teeview_data\"\n",
    "    return teeview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function will go through the new campaigns returned by the \"teeview_scraper\" function\n",
    "# and for each of them request the Teespring campaign in order to find out if it is relevant\n",
    "# or not based on the fact if the campaign reports sales data or not. If it does it adds the\n",
    "# new campaign to the respective database table.\n",
    "def campaigns_updater(teeview_data):\n",
    "    print(\"starting capaigns_updater...\")\n",
    "    # Update the JSON config file if there is new data on teeview\n",
    "    if teeview_data:\n",
    "        update_config(teeview_data[0], int(time.time()))\n",
    "    # Define variables\n",
    "    campaigns = []\n",
    "    # Loop through each link and check if it reports sales data on Teespring campaign page\n",
    "    for campaign_url in teeview_data:\n",
    "        response = requests.get(campaign_url)\n",
    "        html = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # If the html element where sales data is reported exit continue\n",
    "        sales_data_html_el = html.select(\".persistent_timer__order_count\")\n",
    "        if sales_data_html_el:\n",
    "            sales_data_text = sales_data_html_el[0].getText().lower()\n",
    "            # If it reports sales data append campaign to \"campaigns\" variable\n",
    "            if \"only\" in sales_data_text or \"sold\" in sales_data_text:\n",
    "                campaigns.append((campaign_url, html.select(\".campaign__name\")[0].getText(), \"https:\" + html.select(\".image_stack__image\")[0][\"src\"]))\n",
    "    # If new campaigns with reported sales data were found add them to database\n",
    "    if campaigns:\n",
    "        # Connect to \"teeview_analytics\" database\n",
    "        conn = sqlite3.connect('teeview_analytics.db')\n",
    "        # Add filtered campaigns data to \"campaigns\" table\n",
    "        conn.executemany(\"INSERT INTO campaigns(url, name, img) VALUES (?, ?, ?)\", campaigns)\n",
    "        # Commit changed to connection\n",
    "        conn.commit()\n",
    "        # Close connection\n",
    "        conn.close()\n",
    "    print(\"finished capaigns_updater!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will go through the database table that stores the Teespring campaigns that\n",
    "# report sales data and query each of them on Teespring to get the latest info on sales which\n",
    "# will be stored as a new entry on the database table that stores the sales or \"position\" data\n",
    "# of each relevant campaign.\n",
    "def sales_data_updater():\n",
    "    print(\"starting sales_data_updater...\")\n",
    "    # Connect to \"teeview_analytics\" database\n",
    "    conn = sqlite3.connect('teeview_analytics.db')\n",
    "    # Loop through campaigns and update \"sales_data\" table by making the appropriate entries\n",
    "    for row in conn.execute(\"SELECT url FROM campaigns\").fetchall():\n",
    "        # Teespring campaign url\n",
    "        campaign_url = row[0]\n",
    "        # Variables\n",
    "        sales = None\n",
    "        timestamp = int(time.time())\n",
    "        # Make entry with the campaign latest sales reported data\n",
    "        response = requests.get(campaign_url)\n",
    "        html = BeautifulSoup(response.text, \"html.parser\")\n",
    "        sales_data_text = html.select(\".persistent_timer__order_count\")[0].getText()\n",
    "        # Get reported sales data depending on wether goal has been achieved or not yet\n",
    "        if \"only\" in sales_data_text.lower():\n",
    "            sales = -int(sales_data_text.lower().split(\" \")[1])\n",
    "        if \"sold\" in sales_data_text.lower():\n",
    "            sales = int(sales_data_text.lower().split(\" \")[0])\n",
    "        # Make entry in \"sales_data\" table if campaign is reporting sales otherwise append to \"campaign_urls_to_delete\"\n",
    "        if type(sales) == int: \n",
    "            conn.execute(\"INSERT INTO sales_data VALUES ('{0}', {1}, {2})\".format(campaign_url, sales, timestamp))\n",
    "        else:\n",
    "            print(\"ERROR: In sales_data_updater(): Sales reported data is not of type int.\")\n",
    "    # Commit changes to connection\n",
    "    conn.commit()\n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    print(\"finished sales_data_updater!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loop Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this section is to assemble it all together and define the loop that will be runned every 2 hours as defined in the JSON config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop function\n",
    "def do_loop():\n",
    "    # Creates a database if it does not exist already\n",
    "    create_database()\n",
    "    # Cleans up the database associated entries with campaigns that have expired\n",
    "    clean_database()\n",
    "    # Updates the \"campaigns\" with the result from the \"teeview_scraper\"\n",
    "    campaigns_updater(teeview_scraper())\n",
    "    # Updates the \"sales_data\" database table\n",
    "    sales_data_updater()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 23 07:47:13 2017: Starting scrape cycle\n",
      "started creating database...\n",
      "finished creating database!\n",
      "starting cleaning database...\n"
     ]
    }
   ],
   "source": [
    "# Read config data\n",
    "config_data = read_config()\n",
    "# Run loop every 2 hours as defined in the config file variable \"SLEEP_INTERVAL\"\n",
    "while True:\n",
    "    print(\"{}: Starting scrape cycle\".format(time.ctime()))\n",
    "    try:\n",
    "        do_loop()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting....\")\n",
    "        sys.exit(1)\n",
    "    except Exception as exc:\n",
    "        print(\"Error with the scraping:\", sys.exc_info()[0])\n",
    "        traceback.print_exc()\n",
    "    else:\n",
    "        print(\"{0}: Successfully finished scraping\".format(time.ctime()))\n",
    "        time.sleep(config_data[\"SLEEP_INTERVAL\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
